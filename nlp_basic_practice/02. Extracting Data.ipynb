{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the following in this notebook:\n",
    "- 1. Collecting Data from API\n",
    "- 2. Collecting Data from PDFs\n",
    "- 3. Collecting Data from Word files\n",
    "- 4. Collecting Data from JSON\n",
    "- 5. Collecting Data from HTML\n",
    "- 6. Parsing text using Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting Data from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of free APIs through which we can collect data and use it to solve problems. Here, mainly referred to Twitter API in particular since it contains a huge amount of data with a lot of value in it.\n",
    "\n",
    "When all of this data is collected and analyzed, it gives a tremendous amount of insights to a business about the company, product, service etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are needed for Twitter data analysis:\n",
    "- consumer key: Key associated with the application \n",
    "- consumer secret: Password used to authenticate with the authentication server\n",
    "- access token: Key given to the client after successful authentication of above keys\n",
    "- access token secret: Password for the access key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tweepy\n",
    "!pip3 install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "consumer_key = \"wTkUgbPZnCQBKXeZA4jv5KUoX\"\n",
    "consumer_secret = \"Z5hfLPvkyR49q7uFjUf4dDO9Hp65j3YqZ2WY1fHVYvo1GG9FG6\"\n",
    "access_token = \"706794838696574976-s8H9dItuWc1Z8xXakByhXgoSYC8qHha\"\n",
    "access_token_secret = \"RYUI34vZrdArHKigshrQlNQ4wq9inVvmpnDFaGPoRu1RV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling API\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the query you want to pull the data (e.g pulling data for the mobile phone ABC)\n",
    "query = \"ABC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching tweets\n",
    "tweets = api.search(query, count=10, lang='en', exclude='retweets', tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query above will pull the top 10 tweets when the product ABC is searched. The API will pull English tweets since the language given is 'en' and it will exclude retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collecting Data from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time data will be stored as PDF files. We need to extract text from these files and store it for further analysis. Used of PyPDF2 library and see how we can extract data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required library\n",
    "!pip3 install PyPDF2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import a personal pdf file from where I will extract text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pdf file object\n",
    "pdf = open('./data/pdf/Text Classification on Social Media Bachelor Thesis.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pdf reader object\n",
    "pdf_reader = PdfFileReader(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of pages in the pdf file\n",
    "pdf_reader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a page object\n",
    "page = pdf_reader.getPage(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally extract text from the specified page\n",
    "page.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the pdf file\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collecting Data from Word files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to extract data from Word files in Python. For this we will make use of docx library in Python.\n",
    "\n",
    "Used a personal doc file to extract data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required library\n",
    "!pip3 install python-docx --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required library\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word file object\n",
    "file = open('./data/docx/TrustCo System - Analyzing Reviews Report.docx', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word reader object\n",
    "document = Document(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty string and call the document.\n",
    "# document variable stores each paragraph in the Word document. We create a for loop that goes through \n",
    "# each paragraph in the Word document and appends the paragraph.\n",
    "doc = \"\"\n",
    "for p in document.paragraphs:\n",
    "    doc += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collecting Data from JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest solution for reading data from JSON files in Python is by using requests and JSON library provided by Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json from 'https://quotes.rest/qod.json'\n",
    "r = requests.get('https://quotes.rest/qod.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the result into dictionary\n",
    "res = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check json structure\n",
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract contents\n",
    "q = res['contents']['quotes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print output\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract quote and author\n",
    "print(q['quote'], '\\n--', q['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collecting Data from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to collect data from HTML pages. As a solution used of bs4 library also known as BeautifulSoup in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required library\n",
    "!pip3 install bs4 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch html file (e.g Wikipedia)\n",
    "response = urllib.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data into html_doc (binary)\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html file\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the parsed html file\n",
    "strhtml = soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print few lines\n",
    "print(strhtml[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tag values\n",
    "# 1. extract title\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. extract content between the following tags: <title> </title>\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. extract content between the following tags: <a> </a>\n",
    "print(soup.a.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. extract content between the following tags: <b> </b>\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all instances of a particular tag (e.g 'a')\n",
    "content_tag_a = []\n",
    "for x in soup.find_all('a'):\n",
    "    content_tag_a.append(x.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 occurencies \n",
    "content_tag_a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all text of a particular tag\n",
    "content_tag_p = []\n",
    "for x in soup.find_all('p'):\n",
    "    content_tag_p.append(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 occurencies\n",
    "content_tag_p[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parsing text using Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How regular expressions are helpful when dealing with text data. This is very much required when dealing with raw data from the web, which would contain HTML tags, long text, repeated text.\n",
    "\n",
    "For this we will make use of the \"re\" library written in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic flags in \"re\" library are:\n",
    "- re.I : used for ignoring casing\n",
    "- re.L : used for finding a local dependent\n",
    "- re.M : used for finding patterns throughout multiple lines\n",
    "- re.S : used to find dot matches\n",
    "- re.U : used to work for unicode data\n",
    "- re.X : used for writing regex in a more readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression's functionality:\n",
    "- Find the single occurence of character a and b: \n",
    "Regex: [ab]\n",
    "- Find characters except for a and b:\n",
    "Regex: [^ab]\n",
    "- Find the character range of a to z:\n",
    "Regex: [a-z]\n",
    "- Find a range except to z:\n",
    "Regex: [^a-z]\n",
    "- Find all the characters a to z as well A to Z:\n",
    "Regex: [a-zA-Z]\n",
    "- Any single character:\n",
    "Regex:\n",
    "- Any whitespace character:\n",
    "Regex: \\s\n",
    "- Any non-whitespace character:\n",
    "Regex: \\S\n",
    "- Any digit:\n",
    "Regex: \\d\n",
    "- Any non-digit:\n",
    "Regex: \\D\n",
    "- Any words:\n",
    "Regex: \\w\n",
    "- Any non-words:\n",
    "Regex: \\W\n",
    "- Either match a or b:\n",
    "Regex: (a|b)\n",
    "- The occurence of a is either zero or one:\n",
    "    - Matches zero or one occurence but not more than one occurence\n",
    "    Regex: a? ; ?\n",
    "    - The occurence of a is zero times or more than that:\n",
    "    Regex: a* ; * matches zero or more than that\n",
    "    - The occurence of a is one time or more than that:\n",
    "    Regex: a+ ; + matches occurences one or more that one time\n",
    "\n",
    "Exactly match three occurences of a:\n",
    "Regex: a{3}\n",
    "\n",
    "Match simultaneous occurences of a with 3 or more than 3:\n",
    "Regex: a{3,}\n",
    "\n",
    "Match simultaneous occurences of a between 3 to 6:\n",
    "Regex: a{3,6}\n",
    "\n",
    "Starting of the string:\n",
    "Regex: ^\n",
    "\n",
    "Ending of the string:\n",
    "Regex: $\n",
    "\n",
    "Match word boundary:\n",
    "Regex: \\b\n",
    "\n",
    "Non-word boundary:\n",
    "Regex: \\B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most used functions are as follows: re.match() and re.search() and they are used to find patterns, and they can be processed according to the requirements of the application\n",
    "\n",
    "- re.match() function checks for a match of the string only at the beginning of the string\n",
    "                      if it finds the pattern at the beginning of the input string then it\n",
    "                      return matched pattern; else it returns a noun\n",
    "- re.search() function checks for a match of the string anywhere in the string. It finds all\n",
    "                      the occurences of the pattern in the given input string or data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the process of splitting the sentence into chunk of words. One way to do this is by using re.split function from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the split query\n",
    "re.split('\\s+', 'I like this book.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting email IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using re.findall function from Python \"re\" library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read / create the document or sentences\n",
    "doc = \"For more details please mail us at: xyz@abc.com, pqr@mno.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 execute the re.findall function\n",
    "addresses = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for address in addresses:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing email IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we replace  email ids from sentences or documents with another email id. The simplest way to do this is by using re.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read / create the document or sentences\n",
    "doc = \"For more details please mail us at xyz@abc.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. execute re.sub function\n",
    "new_doc = re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)', r'pqr@mno.com', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from ebook and perform regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import re\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url that we want to extract information\n",
    "url = 'https://www.gutenberg.org/files/2638/2638-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract\n",
    "def get_book(url):\n",
    "    # send a http request to get the text from project Gutenberg\n",
    "    raw_text = requests.get(url).text\n",
    "    \n",
    "    # skip metadata from the beginning of the book\n",
    "    start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", raw_text).end()\n",
    "    \n",
    "    # skip metadata from the end of the book\n",
    "    end = re.search(r\"II\", raw_text).start()\n",
    "    \n",
    "    # keep relevant text\n",
    "    text = raw_text[start:end]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(sentence):\n",
    "    return re.sub('[^A-Za-z0-9.]+', ' ', sentence).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = get_book(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing\n",
    "processed_book = preprocess(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_book[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform exploratory data analysis on data using regex\n",
    "# Count number of times \"the\" is appeared in the book\n",
    "len(re.findall(r'the', processed_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"i\" with \"I\"\n",
    "processed_book = re.sub(r'\\si\\s', \" I \", processed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_book[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all occurances of text in the format \"abc--xyz\"\n",
    "re.findall(r'[a-zA-Z0-9]*--[a-zA-Z0-9]*', book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
